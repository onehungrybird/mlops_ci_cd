# ğŸš€ ML Model Deployment with CI/CD (GitHub Actions & AWS)

## ğŸ“Œ Overview
This project demonstrates **end-to-end CI/CD for an ML model**, including:
- **Model Training & Testing** (with unit tests).
- **Containerization & Deployment** (via Docker & GitHub Actions).
- **AWS Deployment** using **SageMaker & ECS (Fargate)**.

---

## âš¡ Features
âœ… **CI/CD Pipeline** using **GitHub Actions**  
âœ… **Automated Model Deployment** to AWS **SageMaker** & **ECS**  
âœ… **Three Deployment Methods:**  
- **Real-Time Endpoint (SageMaker)**
- **Serverless Inference (SageMaker Serverless)**
- **Batch Processing (SageMaker Batch Transform)**  
âœ… **FastAPI-based Model API** for real-time predictions  

---

## ğŸ“‚ Project Structure
```
mlops_ci_cd/
â”‚â”€â”€ .github/workflows/      # CI/CD pipeline setup
â”‚   â”œâ”€â”€ ci_cd.yml           # GitHub Actions workflow
â”‚â”€â”€ data/                   # Training & test data
â”‚â”€â”€ models/                 # Trained model storage
â”‚   â”œâ”€â”€ model.tar.gz        # Model packaged for SageMaker
â”‚â”€â”€ src/                    # ML model & API code
â”‚   â”œâ”€â”€ train.py            # Model training script
â”‚   â”œâ”€â”€ preprocess.py       # Data preprocessing
â”‚   â”œâ”€â”€ app.py              # FastAPI model API
â”‚â”€â”€ tests/                  # Unit tests
â”‚   â”œâ”€â”€ test_train.py       # Test model training
â”‚   â”œâ”€â”€ test_predict.py     # Test API predictions
â”‚â”€â”€ requirements.txt        # Dependencies
â”‚â”€â”€ Dockerfile              # Containerization for AWS ECS
â”‚â”€â”€ README.md               # Project Documentation
â”‚â”€â”€ client.py               # Test API Client
```

---

## ğŸ”§ Installation & Setup

### **1ï¸âƒ£ Clone the Repository**
```sh
git clone https://github.com/onehungrybird/mlops_ci_cd.git
cd mlops_ci_cd
```

### **2ï¸âƒ£ Set Up Python Environment**
```sh
python -m venv venv
source venv/bin/activate   # (Mac/Linux)
venv\Scripts\activate      # (Windows)
pip install -r requirements.txt
```

### **3ï¸âƒ£ Train the Model**
```sh
python src/train.py
```
This will save a trained model inside the **`models/`** directory.

### **4ï¸âƒ£ Run the API Locally**
```sh
uvicorn src.app:app --host 0.0.0.0 --port 8000
```
Test API:
```sh
curl -X POST "http://localhost:8000/predict/" \
     -H "Content-Type: application/json" \
     -d '{"features": [5.1, 3.5, 1.4, 0.2]}'
```

---

## â˜ï¸ **AWS Deployment Methods**
### **1ï¸âƒ£ Deploy as a Real-Time Endpoint (SageMaker)**
```sh
aws s3 cp models/model.tar.gz s3://your-s3-bucket/mlops_model/
python deploy_sagemaker.py
```
âœ… **Best for:** **Low-latency APIs (Chatbots, Recommendations)**  

---

### **2ï¸âƒ£ Deploy as Serverless Inference (SageMaker)**
```sh
python create_serverless.py
```
âœ… **Best for:** **On-demand ML inference (Fraud Detection, OCR)**  
â³ **Has cold start (1-5 sec per request).**  

---

### **3ï¸âƒ£ Deploy as Batch Processing (SageMaker Batch Transform)**
```sh
python run_batch_job.py
```
âœ… **Best for:** **Processing large datasets at once (Customer Churn, Risk Analysis)**  

---

## **ğŸš€ Deploy ML Model to AWS ECS (Fargate)**
### **1ï¸âƒ£ Push Docker Image to AWS ECR**
```sh
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin <aws-account-id>.dkr.ecr.us-east-1.amazonaws.com
docker tag ghcr.io/onehungrybird/mlops_ci_cd:latest <aws-account-id>.dkr.ecr.us-east-1.amazonaws.com/mlops_ci_cd:latest
docker push <aws-account-id>.dkr.ecr.us-east-1.amazonaws.com/mlops_ci_cd:latest
```

### **2ï¸âƒ£ Deploy to AWS ECS**
- **Create ECS Cluster & Task Definition**
- **Deploy ML Model as a service**

Test API after deployment:
```sh
curl -X POST "http://<ecs-public-ip>:8000/predict/" \
     -H "Content-Type: application/json" \
     -d '{"features": [5.1, 3.5, 1.4, 0.2]}'
```

---

## **ğŸ“ Final Takeaways**
âœ” **SageMaker Built-in Images** allow deployment **without Docker**.  
âœ” **Serverless ML** is **cheaper but has cold starts**.  
âœ” **ECS is better if you need a FastAPI-based custom ML API**.  
âœ” **Batch Transform is best for processing large datasets efficiently**.  

---

ğŸš€ **Happy Deploying!** ğŸ”¥
